---
name: gestion-incidents
description: Gestion des incidents en production selon les bonnes pratiques
---

# Gestion des Incidents

Tu g√®res les **incidents en production** de mani√®re structur√©e pour minimiser l'impact et restaurer le service.

## Classification des Incidents

### Matrice de S√©v√©rit√©

| S√©v√©rit√© | Impact | Exemples |
|----------|--------|----------|
| **P1 - Critique** | Service compl√®tement down | Site inaccessible, perte de donn√©es |
| **P2 - Majeur** | Fonctionnalit√© majeure impact√©e | Paiements KO, login impossible |
| **P3 - Mod√©r√©** | Fonctionnalit√© secondaire | Export PDF cass√©, lenteurs |
| **P4 - Mineur** | Impact limit√© | Bug cosm√©tique, edge case |

### Crit√®res de Priorit√©

```
                    Impact Utilisateurs
                    Low    Medium    High
Urgence    High     P3       P2       P1
           Medium   P4       P3       P2
           Low      P4       P4       P3
```

## Processus de Gestion

### Cycle de Vie

```
Detection ‚Üí Triage ‚Üí Investigation ‚Üí Mitigation ‚Üí Resolution ‚Üí Post-Mortem
    ‚îÇ         ‚îÇ           ‚îÇ              ‚îÇ            ‚îÇ            ‚îÇ
    ‚ñº         ‚ñº           ‚ñº              ‚ñº            ‚ñº            ‚ñº
  Alerte   Classify    Debug        Restore      Fix Root      Document
           Assign                   Service       Cause        Learn
```

### Phase 1: D√©tection

| Source | Type |
|--------|------|
| Monitoring | Alertes automatiques (CPU, erreurs, latence) |
| Utilisateurs | Tickets support, signalements |
| √âquipe | Observation lors de tests |
| Partenaires | Signalement API down |

### Phase 2: Triage

```markdown
## Checklist Triage

1. [ ] Confirmer que c'est un vrai incident (pas false positive)
2. [ ] √âvaluer l'impact (scope, utilisateurs, business)
3. [ ] Assigner la s√©v√©rit√© (P1-P4)
4. [ ] Identifier l'Incident Commander
5. [ ] Cr√©er le canal de communication (#incident-YYYY-MM-DD)
6. [ ] Notifier les parties prenantes appropri√©es
```

### Phase 3: Investigation & Mitigation

| Action | Objectif |
|--------|----------|
| Containment | Limiter la propagation |
| Mitigation | Restaurer le service (workaround OK) |
| Root Cause | Identifier la vraie cause |
| Fix | R√©soudre d√©finitivement |

## R√¥les Incident Response

### RACI

| R√¥le | Responsabilit√© |
|------|----------------|
| **Incident Commander (IC)** | Coordonne, d√©cide, communique |
| **Tech Lead** | Investigation technique |
| **Communicator** | Updates stakeholders |
| **Scribe** | Documente la timeline |
| **Subject Matter Expert** | Expertise domaine sp√©cifique |

### Incident Commander

```markdown
## Responsabilit√©s IC

- D√©clarer le d√©but et la fin de l'incident
- Coordonner les efforts de r√©solution
- Prendre les d√©cisions de priorisation
- Autoriser les actions risqu√©es
- Assurer la communication r√©guli√®re
- D√©cider quand escalader
```

## Communication

### Template Notification Initiale

```markdown
üö® **INCIDENT P1 - [Titre Court]**

**Statut**: En cours d'investigation
**D√©but**: HH:MM UTC
**Impact**: [Description impact utilisateurs]
**Affect√©s**: [Scope - tous, r√©gion, segment]

**√âquipe assign√©e**: @on-call @tech-lead
**Canal**: #incident-2024-01-15

Prochain update dans 15 minutes.
```

### Template Update

```markdown
üìä **UPDATE INCIDENT P1 - [Titre]**

**Statut**: [En cours / Mitig√© / R√©solu]
**Dur√©e**: Xh Xmin

**Progr√®s**:
- 10:15 - Cause identifi√©e : [cause]
- 10:20 - Mitigation en cours : [action]

**Prochaines √©tapes**: [actions planifi√©es]

Prochain update dans [X] minutes.
```

### Template R√©solution

```markdown
‚úÖ **R√âSOLU - INCIDENT P1 - [Titre]**

**Dur√©e totale**: Xh Xmin
**Cause**: [r√©sum√© cause racine]
**R√©solution**: [action qui a r√©solu]

**Impact final**:
- Utilisateurs affect√©s: ~X
- Dur√©e d'indisponibilit√©: Xh Xmin

Post-mortem pr√©vu: [date]
```

## Runbooks

### Structure

```markdown
# Runbook: [Nom du Sc√©nario]

## Sympt√¥mes
- Alerte X d√©clench√©e
- Logs montrent Y
- Utilisateurs rapportent Z

## Diagnostic Rapide

\`\`\`bash
# V√©rifier le service
curl -I https://api.example.com/health

# V√©rifier les logs
kubectl logs -f deployment/api --tail=50

# V√©rifier la DB
psql -c "SELECT count(*) FROM pg_stat_activity;"
\`\`\`

## Actions de Mitigation

### Option 1: Restart Service
\`\`\`bash
kubectl rollout restart deployment/api
kubectl rollout status deployment/api
\`\`\`

### Option 2: Rollback
\`\`\`bash
kubectl rollout undo deployment/api
\`\`\`

### Option 3: Scale Up
\`\`\`bash
kubectl scale deployment/api --replicas=5
\`\`\`

## V√©rification

\`\`\`bash
# Confirmer le retour √† la normale
curl https://api.example.com/health
# V√©rifier les m√©triques dans Grafana
\`\`\`

## Escalade

Si non r√©solu apr√®s 15 minutes:
- Escalader √† @tech-lead
- Contacter le support [fournisseur]
```

## Outils

### PagerDuty / OpsGenie

```yaml
# Configuration alerte
alert:
  name: High Error Rate
  condition: error_rate > 1%
  duration: 5m
  severity: P2
  notify:
    - on-call-primary
    - slack:#alerts
  runbook: https://wiki.example.com/runbooks/high-error-rate
```

### Slack Integration

```typescript
// Incident bot commands
/incident create "API Down" P1
/incident update "Identified - DB connection pool exhausted"
/incident resolve "Fixed - Increased pool size"
/incident postmortem create
```

## War Room (P1)

### Setup

```markdown
## Checklist War Room

- [ ] Cr√©er le bridge call (Zoom/Meet permanent)
- [ ] Inviter IC, Tech Lead, SMEs
- [ ] Partager le lien dans #incident
- [ ] Scribe pr√™t √† documenter
- [ ] Dashboard monitoring partag√©
- [ ] Acc√®s aux environnements confirm√©s
```

### R√®gles

| R√®gle | Raison |
|-------|--------|
| IC m√®ne les discussions | √âviter le chaos |
| Un speaker √† la fois | Clart√© |
| Focus sur la mitigation d'abord | Restaurer le service |
| Pas de blame | Psychologie s√©curit√© |
| Documenter en temps r√©el | Post-mortem facilit√© |

## M√©triques

### KPIs Incident Management

| M√©trique | D√©finition | Cible |
|----------|------------|-------|
| **MTTA** | Mean Time To Acknowledge | < 5 min |
| **MTTD** | Mean Time To Detect | < 2 min |
| **MTTR** | Mean Time To Resolve | P1: < 1h |
| **MTBF** | Mean Time Between Failures | > 30 jours |

### Tracking

```sql
-- Dashboard incidents
SELECT
  date_trunc('month', created_at) as month,
  severity,
  count(*) as total,
  avg(extract(epoch from resolved_at - created_at)/60) as avg_mttr_minutes
FROM incidents
WHERE created_at > now() - interval '6 months'
GROUP BY 1, 2
ORDER BY 1 DESC, 2;
```

## Points d'Escalade

| Situation | Action |
|-----------|--------|
| P1 > 30 min sans mitigation | Escalade management |
| Besoin rollback risqu√© | Approbation IC + backup |
| Impact financier majeur | CFO/CEO inform√© |
| Fuite de donn√©es suspect√©e | RSSI + l√©gal |
